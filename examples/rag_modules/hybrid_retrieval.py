"""
æ··åˆæ£€ç´¢æ¨¡å—
åŸºäºåŒå±‚æ£€ç´¢èŒƒå¼ï¼šå®ä½“çº§ + ä¸»é¢˜çº§æ£€ç´¢
ç»“åˆå›¾ç»“æ„æ£€ç´¢å’Œå‘é‡æ£€ç´¢ï¼Œä½¿ç”¨Round-robinè½®è¯¢ç­–ç•¥
"""

import json
import logging
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass

from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever
from neo4j import GraphDatabase
from .graph_indexing import GraphIndexingModule

logger = logging.getLogger(__name__)

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç»“æ„"""
    content: str
    node_id: str
    node_type: str
    relevance_score: float
    retrieval_level: str  # 'low' or 'high'
    metadata: Dict[str, Any]

class HybridRetrievalModule:
    """
    æ··åˆæ£€ç´¢æ¨¡å—
    æ ¸å¿ƒç‰¹ç‚¹ï¼š
    1. åŒå±‚æ£€ç´¢èŒƒå¼ï¼ˆå®ä½“çº§ + ä¸»é¢˜çº§ï¼‰
    2. å…³é”®è¯æå–å’ŒåŒ¹é…
    3. å›¾ç»“æ„+å‘é‡æ£€ç´¢ç»“åˆ
    4. ä¸€è·³é‚»å±…æ‰©å±•
    5. Round-robinè½®è¯¢åˆå¹¶ç­–ç•¥
    """
    
    def __init__(self, config, milvus_module, data_module, llm_client):
        self.config = config
        self.milvus_module = milvus_module
        self.data_module = data_module
        self.llm_client = llm_client
        self.driver = None
        self.bm25_retriever = None
        
        # å›¾ç´¢å¼•æ¨¡å—
        self.graph_indexing = GraphIndexingModule(config, llm_client)
        self.graph_indexed = False
        
    def initialize(self, chunks: List[Document]):
        """åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿ"""
        logger.info("åˆå§‹åŒ–æ··åˆæ£€ç´¢æ¨¡å—...")
        
        # è¿æ¥Neo4j
        self.driver = GraphDatabase.driver(
            self.config.neo4j_uri, 
            auth=(self.config.neo4j_user, self.config.neo4j_password)
        )
        
        # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        if chunks:
            self.bm25_retriever = BM25Retriever.from_documents(chunks)
            logger.info(f"BM25æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(chunks)}")
        
        # åˆå§‹åŒ–å›¾ç´¢å¼•
        self._build_graph_index()
        
    def _build_graph_index(self):
        """æ„å»ºå›¾ç´¢å¼•"""
        if self.graph_indexed:
            return
            
        logger.info("å¼€å§‹æ„å»ºå›¾ç´¢å¼•...")
        
        try:
            # è·å–å›¾æ•°æ®
            recipes = self.data_module.recipes
            ingredients = self.data_module.ingredients
            cooking_steps = self.data_module.cooking_steps
            
            # åˆ›å»ºå®ä½“é”®å€¼å¯¹
            self.graph_indexing.create_entity_key_values(recipes, ingredients, cooking_steps)
            
            # åˆ›å»ºå…³ç³»é”®å€¼å¯¹ï¼ˆè¿™é‡Œéœ€è¦ä»Neo4jè·å–å…³ç³»æ•°æ®ï¼‰
            relationships = self._extract_relationships_from_graph()
            self.graph_indexing.create_relation_key_values(relationships)
            
            # å»é‡ä¼˜åŒ–
            self.graph_indexing.deduplicate_entities_and_relations()
            
            self.graph_indexed = True
            stats = self.graph_indexing.get_statistics()
            logger.info(f"å›¾ç´¢å¼•æ„å»ºå®Œæˆ: {stats}")
            
        except Exception as e:
            logger.error(f"æ„å»ºå›¾ç´¢å¼•å¤±è´¥: {e}")
            
    def _extract_relationships_from_graph(self) -> List[Tuple[str, str, str]]:
        """ä»Neo4jå›¾ä¸­æå–å…³ç³»"""
        relationships = []
        
        try:
            with self.driver.session() as session:
                query = """
                MATCH (source)-[r]->(target)
                WHERE source.nodeId >= '200000000' OR target.nodeId >= '200000000'
                RETURN source.nodeId as source_id, type(r) as relation_type, target.nodeId as target_id
                LIMIT 1000
                """
                result = session.run(query)
                
                for record in result:
                    relationships.append((
                        record["source_id"],
                        record["relation_type"],
                        record["target_id"]
                    ))
                    
        except Exception as e:
            logger.error(f"æå–å›¾å…³ç³»å¤±è´¥: {e}")
            
        return relationships
            
    def extract_query_keywords(self, query: str) -> Tuple[List[str], List[str]]:
        """
        æå–æŸ¥è¯¢å…³é”®è¯ï¼šå®ä½“çº§ + ä¸»é¢˜çº§
        """
        prompt = f"""
        ä½œä¸ºçƒ¹é¥ªçŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·åˆ†æä»¥ä¸‹æŸ¥è¯¢å¹¶æå–å…³é”®è¯ï¼Œåˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡ï¼š

        æŸ¥è¯¢ï¼š{query}

        æå–è§„åˆ™ï¼š
        1. å®ä½“çº§å…³é”®è¯ï¼šå…·ä½“çš„é£Ÿæã€èœå“åç§°ã€å·¥å…·ã€å“ç‰Œç­‰æœ‰å½¢å®ä½“
           - ä¾‹å¦‚ï¼šé¸¡èƒ¸è‚‰ã€è¥¿å…°èŠ±ã€çº¢çƒ§è‚‰ã€å¹³åº•é”…ã€è€å¹²å¦ˆ
           - å¯¹äºæŠ½è±¡æŸ¥è¯¢ï¼Œæ¨æµ‹ç›¸å…³çš„å…·ä½“é£Ÿæ/èœå“

        2. ä¸»é¢˜çº§å…³é”®è¯ï¼šæŠ½è±¡æ¦‚å¿µã€çƒ¹é¥ªä¸»é¢˜ã€é¥®é£Ÿé£æ ¼ã€è¥å…»ç‰¹ç‚¹ç­‰
           - ä¾‹å¦‚ï¼šå‡è‚¥ã€ä½çƒ­é‡ã€å·èœã€ç´ é£Ÿã€ä¸‹é¥­èœã€å¿«æ‰‹èœ
           - æ’é™¤åŠ¨ä½œè¯ï¼šæ¨èã€ä»‹ç»ã€åˆ¶ä½œã€æ€ä¹ˆåšç­‰

        ç¤ºä¾‹ï¼š
        æŸ¥è¯¢ï¼š"æ¨èå‡ ä¸ªå‡è‚¥èœ" 
        {{
            "entity_keywords": ["é¸¡èƒ¸è‚‰", "è¥¿å…°èŠ±", "æ°´ç…®è›‹", "èƒ¡èåœ", "é»„ç“œ"],
            "topic_keywords": ["å‡è‚¥", "ä½çƒ­é‡", "é«˜è›‹ç™½", "ä½è„‚"]
        }}

        æŸ¥è¯¢ï¼š"å·èœæœ‰ä»€ä¹ˆç‰¹è‰²"
        {{
            "entity_keywords": ["éº»å©†è±†è…", "å®«ä¿é¸¡ä¸", "æ°´ç…®é±¼", "è¾£æ¤’", "èŠ±æ¤’"],
            "topic_keywords": ["å·èœ", "éº»è¾£", "é¦™è¾£", "ä¸‹é¥­èœ"]
        }}

        è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«å¤šä½™çš„æ–‡å­—ï¼š
        {{
            "entity_keywords": ["å®ä½“1", "å®ä½“2", ...],
            "topic_keywords": ["ä¸»é¢˜1", "ä¸»é¢˜2", ...]
        }}
        """
        
        try:
            response = self.llm_client.chat.completions.create(
                model=self.config.llm_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=500
            )
            
            result = json.loads(response.choices[0].message.content.strip())
            entity_keywords = result.get("entity_keywords", [])
            topic_keywords = result.get("topic_keywords", [])
            
            logger.info(f"å…³é”®è¯æå–å®Œæˆ - å®ä½“çº§: {entity_keywords}, ä¸»é¢˜çº§: {topic_keywords}")
            return entity_keywords, topic_keywords
            
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            # é™çº§æ–¹æ¡ˆï¼šç®€å•çš„å…³é”®è¯åˆ†å‰²
            keywords = query.split()
            return keywords[:3], keywords[3:6] if len(keywords) > 3 else keywords
    
    def entity_level_retrieval(self, entity_keywords: List[str], top_k: int = 5) -> List[RetrievalResult]:
        """
        å®ä½“çº§æ£€ç´¢ï¼šä¸“æ³¨äºå…·ä½“å®ä½“å’Œå…³ç³»
        ä½¿ç”¨å›¾ç´¢å¼•çš„é”®å€¼å¯¹ç»“æ„è¿›è¡Œæ£€ç´¢
        """
        results = []
        
        # 1. ä½¿ç”¨å›¾ç´¢å¼•è¿›è¡Œå®ä½“æ£€ç´¢
        for keyword in entity_keywords:
            # æ£€ç´¢åŒ¹é…çš„å®ä½“
            entities = self.graph_indexing.get_entities_by_key(keyword)
            
            for entity in entities:
                # è·å–é‚»å±…ä¿¡æ¯
                neighbors = self._get_node_neighbors(entity.metadata["node_id"], max_neighbors=2)
                
                # æ„å»ºå¢å¼ºå†…å®¹
                enhanced_content = entity.value_content
                if neighbors:
                    enhanced_content += f"\nç›¸å…³ä¿¡æ¯: {', '.join(neighbors)}"
                
                results.append(RetrievalResult(
                    content=enhanced_content,
                    node_id=entity.metadata["node_id"],
                    node_type=entity.entity_type,
                    relevance_score=0.9,  # ç²¾ç¡®åŒ¹é…å¾—åˆ†è¾ƒé«˜
                    retrieval_level="entity",
                    metadata={
                        "entity_name": entity.entity_name,
                        "entity_type": entity.entity_type,
                        "index_keys": entity.index_keys,
                        "matched_keyword": keyword
                    }
                ))
        
        # 2. å¦‚æœå›¾ç´¢å¼•ç»“æœä¸è¶³ï¼Œä½¿ç”¨Neo4jè¿›è¡Œè¡¥å……æ£€ç´¢
        if len(results) < top_k:
            neo4j_results = self._neo4j_entity_level_search(entity_keywords, top_k - len(results))
            results.extend(neo4j_results)
            
        # 3. æŒ‰ç›¸å…³æ€§æ’åºå¹¶è¿”å›
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        logger.info(f"å®ä½“çº§æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
        return results[:top_k]
    
    def _neo4j_entity_level_search(self, keywords: List[str], limit: int) -> List[RetrievalResult]:
        """Neo4jè¡¥å……æ£€ç´¢"""
        results = []
        
        try:
            with self.driver.session() as session:
                cypher_query = """
                UNWIND $keywords as keyword
                CALL db.index.fulltext.queryNodes('recipe_fulltext_index', keyword + '*') 
                YIELD node, score
                WHERE node:Recipe
                RETURN 
                    node.nodeId as node_id,
                    node.name as name,
                    node.description as description,
                    labels(node) as labels,
                    score
                ORDER BY score DESC
                LIMIT $limit
                """
                
                result = session.run(cypher_query, {
                    "keywords": keywords,
                    "limit": limit
                })
                
                for record in result:
                    content_parts = []
                    if record["name"]:
                        content_parts.append(f"èœå“: {record['name']}")
                    if record["description"]:
                        content_parts.append(f"æè¿°: {record['description']}")
                    
                    results.append(RetrievalResult(
                        content='\n'.join(content_parts),
                        node_id=record["node_id"],
                        node_type="Recipe",
                        relevance_score=float(record["score"]) * 0.7,  # è¡¥å……æ£€ç´¢å¾—åˆ†è¾ƒä½
                        retrieval_level="entity",
                        metadata={
                            "name": record["name"],
                            "labels": record["labels"],
                            "source": "neo4j_fallback"
                        }
                    ))
                    
        except Exception as e:
            logger.error(f"Neo4jè¡¥å……æ£€ç´¢å¤±è´¥: {e}")
            
        return results
    
    def topic_level_retrieval(self, topic_keywords: List[str], top_k: int = 5) -> List[RetrievalResult]:
        """
        ä¸»é¢˜çº§æ£€ç´¢ï¼šä¸“æ³¨äºå¹¿æ³›ä¸»é¢˜å’Œæ¦‚å¿µ
        ä½¿ç”¨å›¾ç´¢å¼•çš„å…³ç³»é”®å€¼å¯¹ç»“æ„è¿›è¡Œä¸»é¢˜æ£€ç´¢
        """
        results = []
        
        # 1. ä½¿ç”¨å›¾ç´¢å¼•è¿›è¡Œå…³ç³»/ä¸»é¢˜æ£€ç´¢
        for keyword in topic_keywords:
            # æ£€ç´¢åŒ¹é…çš„å…³ç³»
            relations = self.graph_indexing.get_relations_by_key(keyword)
            
            for relation in relations:
                # è·å–ç›¸å…³å®ä½“ä¿¡æ¯
                source_entity = self.graph_indexing.entity_kv_store.get(relation.source_entity)
                target_entity = self.graph_indexing.entity_kv_store.get(relation.target_entity)
                
                if source_entity and target_entity:
                    # æ„å»ºä¸°å¯Œçš„ä¸»é¢˜å†…å®¹
                    content_parts = [
                        f"ä¸»é¢˜: {keyword}",
                        relation.value_content,
                        f"ç›¸å…³èœå“: {source_entity.entity_name}",
                        f"ç›¸å…³ä¿¡æ¯: {target_entity.entity_name}"
                    ]
                    
                    # æ·»åŠ æºå®ä½“çš„è¯¦ç»†ä¿¡æ¯
                    if source_entity.entity_type == "Recipe":
                        newline = '\n'
                        content_parts.append(f"èœå“è¯¦æƒ…: {source_entity.value_content.split(newline)[0]}")
                    
                    results.append(RetrievalResult(
                        content='\n'.join(content_parts),
                        node_id=relation.source_entity,  # ä»¥ä¸»è¦å®ä½“ä¸ºID
                        node_type=source_entity.entity_type,
                        relevance_score=0.95,  # ä¸»é¢˜åŒ¹é…å¾—åˆ†
                        retrieval_level="topic",
                        metadata={
                            "relation_id": relation.relation_id,
                            "relation_type": relation.relation_type,
                            "source_name": source_entity.entity_name,
                            "target_name": target_entity.entity_name,
                            "matched_keyword": keyword,
                            "index_keys": relation.index_keys
                        }
                    ))
        
        # 2. ä½¿ç”¨å®ä½“çš„åˆ†ç±»ä¿¡æ¯è¿›è¡Œä¸»é¢˜æ£€ç´¢
        for keyword in topic_keywords:
            entities = self.graph_indexing.get_entities_by_key(keyword)
            for entity in entities:
                if entity.entity_type == "Recipe":
                    # æ„å»ºåˆ†ç±»ä¸»é¢˜å†…å®¹
                    content_parts = [
                        f"ä¸»é¢˜åˆ†ç±»: {keyword}",
                        entity.value_content
                    ]
                    
                    results.append(RetrievalResult(
                        content='\n'.join(content_parts),
                        node_id=entity.metadata["node_id"],
                        node_type=entity.entity_type,
                        relevance_score=0.85,  # åˆ†ç±»åŒ¹é…å¾—åˆ†
                        retrieval_level="topic",
                        metadata={
                            "entity_name": entity.entity_name,
                            "entity_type": entity.entity_type,
                            "matched_keyword": keyword,
                            "source": "category_match"
                        }
                    ))
        
        # 3. å¦‚æœç»“æœä¸è¶³ï¼Œä½¿ç”¨Neo4jè¿›è¡Œè¡¥å……æ£€ç´¢
        if len(results) < top_k:
            neo4j_results = self._neo4j_topic_level_search(topic_keywords, top_k - len(results))
            results.extend(neo4j_results)
            
        # 4. æŒ‰ç›¸å…³æ€§æ’åºå¹¶è¿”å›
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        
        logger.info(f"ä¸»é¢˜çº§æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
        return results[:top_k]
    
    def _neo4j_topic_level_search(self, keywords: List[str], limit: int) -> List[RetrievalResult]:
        """Neo4jä¸»é¢˜çº§æ£€ç´¢è¡¥å……"""
        results = []
        
        try:
            with self.driver.session() as session:
                cypher_query = """
                UNWIND $keywords as keyword
                MATCH (r:Recipe)
                WHERE r.category CONTAINS keyword 
                   OR r.cuisineType CONTAINS keyword
                   OR r.tags CONTAINS keyword
                WITH r, keyword
                OPTIONAL MATCH (r)-[:REQUIRES]->(i:Ingredient)
                WITH r, keyword, collect(i.name)[0..3] as ingredients
                RETURN 
                    r.nodeId as node_id,
                    r.name as name,
                    r.category as category,
                    r.cuisineType as cuisine_type,
                    r.difficulty as difficulty,
                    ingredients,
                    keyword as matched_keyword
                ORDER BY r.difficulty ASC, r.name
                LIMIT $limit
                """
                
                result = session.run(cypher_query, {
                    "keywords": keywords,
                    "limit": limit
                })
                
                for record in result:
                    content_parts = []
                    content_parts.append(f"èœå“: {record['name']}")
                    
                    if record["category"]:
                        content_parts.append(f"åˆ†ç±»: {record['category']}")
                    if record["cuisine_type"]:
                        content_parts.append(f"èœç³»: {record['cuisine_type']}")
                    if record["difficulty"]:
                        content_parts.append(f"éš¾åº¦: {record['difficulty']}")
                    
                    if record["ingredients"]:
                        ingredients_str = ', '.join(record["ingredients"][:3])
                        content_parts.append(f"ä¸»è¦é£Ÿæ: {ingredients_str}")
                    
                    results.append(RetrievalResult(
                        content='\n'.join(content_parts),
                        node_id=record["node_id"],
                        node_type="Recipe",
                        relevance_score=0.75,  # è¡¥å……æ£€ç´¢å¾—åˆ†
                        retrieval_level="topic",
                        metadata={
                            "name": record["name"],
                            "category": record["category"],
                            "cuisine_type": record["cuisine_type"],
                            "difficulty": record["difficulty"],
                            "matched_keyword": record["matched_keyword"],
                            "source": "neo4j_fallback"
                        }
                    ))
                    
        except Exception as e:
            logger.error(f"Neo4jä¸»é¢˜çº§æ£€ç´¢å¤±è´¥: {e}")
            
        return results
        
    def dual_level_retrieval(self, query: str, top_k: int = 5) -> List[Document]:
        """
        åŒå±‚æ£€ç´¢ï¼šç»“åˆå®ä½“çº§å’Œä¸»é¢˜çº§æ£€ç´¢
        """
        logger.info(f"å¼€å§‹åŒå±‚æ£€ç´¢: {query}")
        
        # 1. æå–å…³é”®è¯
        entity_keywords, topic_keywords = self.extract_query_keywords(query)
        
        # 2. æ‰§è¡ŒåŒå±‚æ£€ç´¢
        entity_results = self.entity_level_retrieval(entity_keywords, top_k)
        topic_results = self.topic_level_retrieval(topic_keywords, top_k)
        
        # 3. ç»“æœåˆå¹¶å’Œæ’åº
        all_results = entity_results + topic_results
        
        # 4. å»é‡å’Œé‡æ’åº
        seen_nodes = set()
        unique_results = []
        
        for result in sorted(all_results, key=lambda x: x.relevance_score, reverse=True):
            if result.node_id not in seen_nodes:
                seen_nodes.add(result.node_id)
                unique_results.append(result)
        
        # 5. è½¬æ¢ä¸ºDocumentæ ¼å¼
        documents = []
        for result in unique_results[:top_k]:
            # ç¡®ä¿recipe_nameå­—æ®µæ­£ç¡®è®¾ç½®
            recipe_name = result.metadata.get("name") or result.metadata.get("entity_name", "æœªçŸ¥èœå“")
            
            doc = Document(
                page_content=result.content,
                metadata={
                    "node_id": result.node_id,
                    "node_type": result.node_type,
                    "retrieval_level": result.retrieval_level,
                    "relevance_score": result.relevance_score,
                    "recipe_name": recipe_name,  # ç¡®ä¿æœ‰recipe_nameå­—æ®µ
                    "search_type": "dual_level",  # è®¾ç½®æœç´¢ç±»å‹
                    **result.metadata
                }
            )
            documents.append(doc)
            
        logger.info(f"åŒå±‚æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def vector_search_enhanced(self, query: str, top_k: int = 5) -> List[Document]:
        """
        å¢å¼ºçš„å‘é‡æ£€ç´¢ï¼šç»“åˆå›¾ä¿¡æ¯
        """
        try:
            # ä½¿ç”¨Milvusè¿›è¡Œå‘é‡æ£€ç´¢
            vector_docs = self.milvus_module.similarity_search(query, k=top_k*2)
            
            # ç”¨å›¾ä¿¡æ¯å¢å¼ºç»“æœå¹¶è½¬æ¢ä¸ºDocumentå¯¹è±¡
            enhanced_docs = []
            for result in vector_docs:
                # ä»Milvusç»“æœåˆ›å»ºDocumentå¯¹è±¡
                content = result.get("text", "")
                metadata = result.get("metadata", {})
                node_id = metadata.get("node_id")
                
                if node_id:
                    # ä»å›¾ä¸­è·å–é‚»å±…ä¿¡æ¯
                    neighbors = self._get_node_neighbors(node_id)
                    if neighbors:
                        # å°†é‚»å±…ä¿¡æ¯æ·»åŠ åˆ°å†…å®¹ä¸­
                        neighbor_info = f"\nç›¸å…³ä¿¡æ¯: {', '.join(neighbors[:3])}"
                        content += neighbor_info
                
                # ç¡®ä¿recipe_nameå­—æ®µæ­£ç¡®è®¾ç½®
                recipe_name = metadata.get("recipe_name", "æœªçŸ¥èœå“")
                
                # è°ƒè¯•ï¼šæ‰“å°å‘é‡å¾—åˆ†
                vector_score = result.get("score", 0.0)
                logger.debug(f"å‘é‡æ£€ç´¢å¾—åˆ†: {recipe_name} = {vector_score}")
                
                # åˆ›å»ºDocumentå¯¹è±¡
                doc = Document(
                    page_content=content,
                    metadata={
                        **metadata,
                        "recipe_name": recipe_name,  # ç¡®ä¿æœ‰recipe_nameå­—æ®µ
                        "score": vector_score,
                        "search_type": "vector_enhanced"
                    }
                )
                enhanced_docs.append(doc)
                
            return enhanced_docs[:top_k]
            
        except Exception as e:
            logger.error(f"å¢å¼ºå‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _get_node_neighbors(self, node_id: str, max_neighbors: int = 3) -> List[str]:
        """è·å–èŠ‚ç‚¹çš„é‚»å±…ä¿¡æ¯"""
        try:
            with self.driver.session() as session:
                query = """
                MATCH (n {nodeId: $node_id})-[r]-(neighbor)
                RETURN neighbor.name as name
                LIMIT $limit
                """
                result = session.run(query, {"node_id": node_id, "limit": max_neighbors})
                return [record["name"] for record in result if record["name"]]
        except Exception as e:
            logger.error(f"è·å–é‚»å±…èŠ‚ç‚¹å¤±è´¥: {e}")
            return []
    
    def hybrid_search(self, query: str, top_k: int = 5) -> List[Document]:
        """
        æ··åˆæ£€ç´¢ï¼šå¹¶è¡Œæ‰§è¡Œå¤šç§æ£€ç´¢ç­–ç•¥
        """
        import concurrent.futures

        logger.info(f"å¼€å§‹å¹¶è¡Œæ··åˆæ£€ç´¢: {query}")

        # ğŸš€ å¹¶è¡Œæ‰§è¡Œä¸åŒæ£€ç´¢ç­–ç•¥
        dual_docs = []
        vector_docs = []

        def dual_search():
            nonlocal dual_docs
            try:
                dual_docs = self.dual_level_retrieval(query, top_k)
                logger.info(f"åŒå±‚æ£€ç´¢å®Œæˆ: {len(dual_docs)} ä¸ªç»“æœ")
            except Exception as e:
                logger.error(f"åŒå±‚æ£€ç´¢å¤±è´¥: {e}")
                dual_docs = []

        def vector_search():
            nonlocal vector_docs
            try:
                vector_docs = self.vector_search_enhanced(query, top_k)
                logger.info(f"å‘é‡æ£€ç´¢å®Œæˆ: {len(vector_docs)} ä¸ªç»“æœ")
            except Exception as e:
                logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
                vector_docs = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            future_dual = executor.submit(dual_search)
            future_vector = executor.submit(vector_search)

            # ç­‰å¾…æ£€ç´¢å®Œæˆ
            concurrent.futures.wait([future_dual, future_vector], timeout=20)

        # 3. Round-robinè½®è¯¢åˆå¹¶
        merged_docs = []
        seen_doc_ids = set()
        max_len = max(len(dual_docs), len(vector_docs))
        origin_len = len(dual_docs) + len(vector_docs)
        
        for i in range(max_len):
            # å…ˆæ·»åŠ åŒå±‚æ£€ç´¢ç»“æœ
            if i < len(dual_docs):
                doc = dual_docs[i]
                doc_id = doc.metadata.get("node_id", hash(doc.page_content))
                if doc_id not in seen_doc_ids:
                    seen_doc_ids.add(doc_id)
                    doc.metadata["search_method"] = "dual_level"
                    doc.metadata["round_robin_order"] = len(merged_docs)
                    # è®¾ç½®ç»Ÿä¸€çš„final_scoreå­—æ®µ
                    doc.metadata["final_score"] = doc.metadata.get("relevance_score", 0.0)
                    merged_docs.append(doc)
            
            # å†æ·»åŠ å‘é‡æ£€ç´¢ç»“æœ
            if i < len(vector_docs):
                doc = vector_docs[i]
                doc_id = doc.metadata.get("node_id", hash(doc.page_content))
                if doc_id not in seen_doc_ids:
                    seen_doc_ids.add(doc_id)
                    doc.metadata["search_method"] = "vector_enhanced"
                    doc.metadata["round_robin_order"] = len(merged_docs)
                    # è®¾ç½®ç»Ÿä¸€çš„final_scoreå­—æ®µï¼ˆå‘é‡å¾—åˆ†éœ€è¦è½¬æ¢ï¼‰
                    vector_score = doc.metadata.get("score", 0.0)
                    # COSINEè·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼šdistanceè¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
                    similarity_score = max(0.0, 1.0 - vector_score) if vector_score <= 1.0 else 0.0
                    doc.metadata["final_score"] = similarity_score
                    merged_docs.append(doc)
        
        # å–å‰top_kä¸ªç»“æœ
        final_docs = merged_docs[:top_k]
        
        logger.info(f"Round-robinåˆå¹¶ï¼šä»æ€»å…±{origin_len}ä¸ªç»“æœåˆå¹¶ä¸º{len(final_docs)}ä¸ªæ–‡æ¡£")
        logger.info(f"æ··åˆæ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_docs)} ä¸ªæ–‡æ¡£")
        return final_docs
        
    def close(self):
        """å…³é—­èµ„æºè¿æ¥"""
        if self.driver:
            self.driver.close()
            logger.info("Neo4jè¿æ¥å·²å…³é—­") 